{"version":3,"sources":["../src/index.ts"],"sourcesContent":["#!/usr/bin/env node\nimport {\n    convert,\n    Generate,\n    Llm,\n    ModelLoad,\n    InferenceResultType,\n} from \"@llama-node/core\";\nimport yargs from \"yargs\";\nimport path from \"path\";\nimport { existsSync } from \"fs\";\n\nconst convertType = [\"q4_0\", \"q4_1\", \"f16\", \"f32\"] as const;\n\ntype ConvertType = (typeof convertType)[number];\n\ninterface CLIInferenceArguments extends Partial<Generate>, ModelLoad {\n    logger?: boolean;\n}\n\nclass InferenceCommand implements yargs.CommandModule {\n    command = \"inference\";\n    describe = \"Inference LLaMA\";\n    builder(args: yargs.Argv) {\n        return (args as yargs.Argv<CLIInferenceArguments>)\n            .help(\"help\")\n            .example('llama inference -p \"How are you?\"', \"Inference LLaMA\")\n            .options(\"modelType\", {\n                type: \"string\",\n                demandOption: true,\n            })\n            .options(\"feedPrompt\", {\n                type: \"boolean\",\n                demandOption: false,\n                description: \"Set it to true to hide promt feeding progress\",\n            })\n            .options(\"float16\", { type: \"boolean\", demandOption: false })\n            .options(\"ignoreEos\", { type: \"boolean\", demandOption: false })\n            .options(\"batchSize\", { type: \"number\", demandOption: false })\n            .options(\"numThreads\", { type: \"number\", demandOption: false })\n            .options(\"numPredict\", { type: \"number\", demandOption: false })\n            .options(\"prompt\", {\n                type: \"string\",\n                demandOption: true,\n                alias: \"p\",\n            })\n            .options(\"repeatLastN\", { type: \"number\", demandOption: false })\n            .options(\"repeatPenalty\", { type: \"number\", demandOption: false })\n            .options(\"seed\", { type: \"number\", demandOption: false })\n            .options(\"temperature\", { type: \"number\", demandOption: false })\n            .options(\"tokenBias\", { type: \"string\", demandOption: false })\n            .options(\"topK\", { type: \"number\", demandOption: false })\n            .options(\"topP\", { type: \"number\", demandOption: false })\n            .options(\"modelPath\", {\n                type: \"string\",\n                demandOption: true,\n                alias: [\"m\", \"model\"],\n            })\n            .options(\"numCtxTokens\", { type: \"number\", demandOption: false })\n            .options(\"logger\", {\n                type: \"boolean\",\n                demandOption: false,\n                default: true,\n                alias: \"verbose\",\n            });\n    }\n    async handler(args: yargs.ArgumentsCamelCase) {\n        const { $0, _, modelPath, modelType, numCtxTokens, logger, ...rest } =\n            args as yargs.ArgumentsCamelCase<CLIInferenceArguments>;\n        const absolutePath = path.isAbsolute(modelPath)\n            ? modelPath\n            : path.join(process.cwd(), modelPath);\n        const llm = await Llm.load(\n            {\n                modelPath: absolutePath,\n                modelType,\n                numCtxTokens,\n            },\n            logger ?? true\n        );\n        llm.inference(rest, (result) => {\n            switch (result.type) {\n                case InferenceResultType.Data:\n                    process.stdout.write(result.data?.token ?? \"\");\n                    break;\n                case InferenceResultType.Error:\n                    console.error(result.message);\n                    break;\n                case InferenceResultType.End:\n                    break;\n            }\n        });\n    }\n}\n\n(yargs as yargs.Argv<any | CLIInferenceArguments>)\n    .scriptName(\"llama\")\n    .usage(\"$0 <cmd> [args]\")\n    .command(new InferenceCommand())\n    .demandCommand(1, \"You need at least one command before moving on\")\n    .strict()\n    .parse();\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AACA,kBAMO;AACP,mBAAkB;AAClB,kBAAiB;AAWjB,IAAM,mBAAN,MAAsD;AAAA,EAAtD;AACI,mBAAU;AACV,oBAAW;AAAA;AAAA,EACX,QAAQ,MAAkB;AACtB,WAAQ,KACH,KAAK,MAAM,EACX,QAAQ,qCAAqC,iBAAiB,EAC9D,QAAQ,aAAa;AAAA,MAClB,MAAM;AAAA,MACN,cAAc;AAAA,IAClB,CAAC,EACA,QAAQ,cAAc;AAAA,MACnB,MAAM;AAAA,MACN,cAAc;AAAA,MACd,aAAa;AAAA,IACjB,CAAC,EACA,QAAQ,WAAW,EAAE,MAAM,WAAW,cAAc,MAAM,CAAC,EAC3D,QAAQ,aAAa,EAAE,MAAM,WAAW,cAAc,MAAM,CAAC,EAC7D,QAAQ,aAAa,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC5D,QAAQ,cAAc,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC7D,QAAQ,cAAc,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC7D,QAAQ,UAAU;AAAA,MACf,MAAM;AAAA,MACN,cAAc;AAAA,MACd,OAAO;AAAA,IACX,CAAC,EACA,QAAQ,eAAe,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC9D,QAAQ,iBAAiB,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAChE,QAAQ,QAAQ,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EACvD,QAAQ,eAAe,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC9D,QAAQ,aAAa,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC5D,QAAQ,QAAQ,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EACvD,QAAQ,QAAQ,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EACvD,QAAQ,aAAa;AAAA,MAClB,MAAM;AAAA,MACN,cAAc;AAAA,MACd,OAAO,CAAC,KAAK,OAAO;AAAA,IACxB,CAAC,EACA,QAAQ,gBAAgB,EAAE,MAAM,UAAU,cAAc,MAAM,CAAC,EAC/D,QAAQ,UAAU;AAAA,MACf,MAAM;AAAA,MACN,cAAc;AAAA,MACd,SAAS;AAAA,MACT,OAAO;AAAA,IACX,CAAC;AAAA,EACT;AAAA,EACM,QAAQ,MAAgC;AAAA;AAC1C,YACI,WADI,MAAI,GAAG,WAAW,WAAW,cAAc,OAnE3D,IAoEY,IAD0D,iBAC1D,IAD0D,CAAtD,MAAI,KAAG,aAAW,aAAW,gBAAc;AAEnD,YAAM,eAAe,YAAAA,QAAK,WAAW,SAAS,IACxC,YACA,YAAAA,QAAK,KAAK,QAAQ,IAAI,GAAG,SAAS;AACxC,YAAM,MAAM,MAAM,gBAAI;AAAA,QAClB;AAAA,UACI,WAAW;AAAA,UACX;AAAA,UACA;AAAA,QACJ;AAAA,QACA,0BAAU;AAAA,MACd;AACA,UAAI,UAAU,MAAM,CAAC,WAAW;AAhFxC,YAAAC,KAAA;AAiFY,gBAAQ,OAAO,MAAM;AAAA,UACjB,KAAK,gCAAoB;AACrB,oBAAQ,OAAO,OAAM,MAAAA,MAAA,OAAO,SAAP,gBAAAA,IAAa,UAAb,YAAsB,EAAE;AAC7C;AAAA,UACJ,KAAK,gCAAoB;AACrB,oBAAQ,MAAM,OAAO,OAAO;AAC5B;AAAA,UACJ,KAAK,gCAAoB;AACrB;AAAA,QACR;AAAA,MACJ,CAAC;AAAA,IACL;AAAA;AACJ;AAEC,aAAAC,QACI,WAAW,OAAO,EAClB,MAAM,iBAAiB,EACvB,QAAQ,IAAI,iBAAiB,CAAC,EAC9B,cAAc,GAAG,gDAAgD,EACjE,OAAO,EACP,MAAM;","names":["path","_a","yargs"]}