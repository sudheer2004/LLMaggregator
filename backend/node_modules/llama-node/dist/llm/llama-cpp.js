var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __objRest = (source, exclude) => {
  var target = {};
  for (var prop in source)
    if (__hasOwnProp.call(source, prop) && exclude.indexOf(prop) < 0)
      target[prop] = source[prop];
  if (source != null && __getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(source)) {
      if (exclude.indexOf(prop) < 0 && __propIsEnum.call(source, prop))
        target[prop] = source[prop];
    }
  return target;
};
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};

// src/llm/llama-cpp.ts
import {
  InferenceResultType,
  LLama
} from "@llama-node/llama-cpp";

// src/llm/type.ts
var LLMError = class extends Error {
  constructor({
    message,
    tokens,
    completed,
    type
  }) {
    super(message);
    this.tokens = tokens;
    this.completed = completed;
    this.type = type;
  }
};

// src/llm/llama-cpp.ts
var LLamaCpp = class {
  load(config) {
    return __async(this, null, function* () {
      const _a = config, { enableLogging } = _a, rest = __objRest(_a, ["enableLogging"]);
      this.instance = yield LLama.load(rest, enableLogging);
    });
  }
  createCompletion(params, callback, abortSignal) {
    return __async(this, null, function* () {
      let completed = false;
      const tokens = [];
      const errors = [];
      return new Promise(
        (res, rej) => {
          const abort = this.instance.inference(params, (response) => {
            var _a;
            switch (response.type) {
              case InferenceResultType.Data: {
                const data = {
                  token: response.data.token,
                  completed: !!response.data.completed
                };
                tokens.push(data.token);
                if (data.completed) {
                  completed = true;
                }
                callback(data);
                break;
              }
              case InferenceResultType.End: {
                if (errors.length) {
                  rej(
                    new LLMError({
                      message: errors.join("\n"),
                      tokens,
                      completed,
                      type: "Generic" /* Generic */
                    })
                  );
                } else {
                  res({ tokens, completed });
                }
                break;
              }
              case InferenceResultType.Error: {
                errors.push((_a = response.message) != null ? _a : "Unknown Error");
                break;
              }
            }
          });
          const abortSignalHandler = () => {
            abort();
            rej(
              new LLMError({
                message: "Aborted",
                tokens,
                completed,
                type: "Aborted" /* Aborted */
              })
            );
            abortSignal == null ? void 0 : abortSignal.removeEventListener(
              "abort",
              abortSignalHandler
            );
          };
          abortSignal == null ? void 0 : abortSignal.addEventListener("abort", abortSignalHandler);
        }
      );
    });
  }
  getEmbedding(params) {
    return __async(this, null, function* () {
      return yield this.instance.getWordEmbedding(params);
    });
  }
  getDefaultEmbedding(text) {
    return __async(this, null, function* () {
      return this.getEmbedding({
        nThreads: 4,
        nTokPredict: 1024,
        topK: 40,
        topP: 0.1,
        temp: 0.1,
        repeatPenalty: 1,
        prompt: text
      });
    });
  }
  tokenize(params) {
    return __async(this, null, function* () {
      return yield this.instance.tokenize(params);
    });
  }
};
export {
  LLamaCpp
};
//# sourceMappingURL=llama-cpp.js.map