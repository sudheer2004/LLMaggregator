import { ModelLoad, LLama, Generate } from '@llama-node/llama-cpp';
import { ILLM, LLMResult } from './type.js';

interface LoadConfig extends ModelLoad {
    enableLogging: boolean;
}
declare class LLamaCpp implements ILLM<LLama, LoadConfig, Generate, Generate, string> {
    instance: LLama;
    load(config: LoadConfig): Promise<void>;
    createCompletion(params: Generate, callback: (data: {
        token: string;
        completed: boolean;
    }) => void, abortSignal?: AbortSignal): Promise<LLMResult>;
    getEmbedding(params: Generate): Promise<number[]>;
    getDefaultEmbedding(text: string): Promise<number[]>;
    tokenize(params: string): Promise<number[]>;
}

export { LLamaCpp, LoadConfig };
